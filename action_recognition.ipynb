{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 123\n",
    "# Numpy\n",
    "np.random.seed(seed)\n",
    "# Pytorch\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(\"animal_data/data.npy\")\n",
    "label = np.load(\"animal_data/label.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38494, 3, 60, 15)\n",
      "(38494,)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(label[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23096.399999999998\n",
      "30795.2\n",
      "23096\n",
      "30795\n"
     ]
    }
   ],
   "source": [
    "print(data.shape[0]*0.6)\n",
    "print(data.shape[0]*0.8)\n",
    "print(round(data.shape[0]*0.6))\n",
    "print(round(data.shape[0]*0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:round(data.shape[0]*0.6)]\n",
    "val_data = data[round(data.shape[0]*0.6):round(data.shape[0]*0.8)]\n",
    "test_data = data[round(data.shape[0]*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = label[:round(label.shape[0]*0.6)]\n",
    "val_label = label[round(label.shape[0]*0.6):round(label.shape[0]*0.8)]\n",
    "test_label = label[round(label.shape[0]*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('animal_data/train_data',train_data)\n",
    "np.save('animal_data/val_data',val_data)\n",
    "np.save('animal_data/test_data',test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('animal_data/train_label',train_label)\n",
    "np.save('animal_data/val_label',val_label)\n",
    "np.save('animal_data/test_label',test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"animal_data/train_data.npy\")\n",
    "val_data = np.load(\"animal_data/val_data.npy\")\n",
    "test_data = np.load(\"animal_data/test_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = np.load(\"animal_data/train_label.npy\")\n",
    "val_label = np.load(\"animal_data/val_label.npy\")\n",
    "test_label = np.load(\"animal_data/test_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23096, 3, 60, 15)\n",
      "(7699, 3, 60, 15)\n",
      "(7699, 3, 60, 15)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23096,)\n",
      "(7699,)\n",
      "(7699,)\n"
     ]
    }
   ],
   "source": [
    "print(train_label.shape)\n",
    "print(val_label.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 읽기\n",
    "class Feeder(torch.utils.data.Dataset):\n",
    "  def __init__(self, data_path, label_path):\n",
    "      super().__init__()\n",
    "      self.label = np.load(label_path)\n",
    "      self.data = np.load(data_path)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.label)\n",
    "\n",
    "  def __iter__(self):\n",
    "      return self\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      data = np.array(self.data[index])\n",
    "      label = self.label[index]\n",
    "\n",
    "      return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#연결 관계를 정의하고 그래프로 만듦\n",
    "#연결 관계를 배열로 만든 후 인접 행렬 생성 \n",
    "\n",
    "class Graph():\n",
    "  def __init__(self, hop_size):\n",
    "    #엣지 배열 선언 \n",
    "    self.get_edge()\n",
    "\n",
    "    #hop: 노드 간의 연결을 나타내는 단위거리 \n",
    "    #hop = 2이면 손목을 팔꿈치와 어깨와 연결\n",
    "    self.hop_size = hop_size \n",
    "    self.hop_dis = self.get_hop_distance(self.num_node, self.edge, hop_size=hop_size)\n",
    "    #hop 수마다 인접행렬 생성\n",
    "    self.get_adjacency() \n",
    "\n",
    "  def __str__(self):\n",
    "    return self.A\n",
    "\n",
    "  def get_edge(self):\n",
    "    self.num_node = 15\n",
    "    self_link = [(i, i) for i in range(self.num_node)]\n",
    "    neighbor_base = [\n",
    "          (0, 1),  (0,3), (2,3), (3, 4), (4, 5),\n",
    "          (4, 6), (5, 7), (6, 8), (4, 13), (13, 9), \n",
    "          (13, 10), (13,14), (9, 11), (10,12)\n",
    "    ]\n",
    "    neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_base]\n",
    "    self.edge = self_link + neighbor_link\n",
    "\n",
    "\n",
    "  def get_adjacency(self):\n",
    "    valid_hop = range(0, self.hop_size + 1, 1)\n",
    "    adjacency = np.zeros((self.num_node, self.num_node))\n",
    "    for hop in valid_hop:\n",
    "        adjacency[self.hop_dis == hop] = 1\n",
    "    normalize_adjacency = self.normalize_digraph(adjacency)\n",
    "    A = np.zeros((len(valid_hop), self.num_node, self.num_node))\n",
    "    for i, hop in enumerate(valid_hop):\n",
    "        A[i][self.hop_dis == hop] = normalize_adjacency[self.hop_dis == hop]\n",
    "    self.A = A\n",
    "\n",
    "  def get_hop_distance(self, num_node, edge, hop_size):\n",
    "    A = np.zeros((num_node, num_node))\n",
    "    for i, j in edge:\n",
    "        A[j, i] = 1\n",
    "        A[i, j] = 1\n",
    "    hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
    "    transfer_mat = [np.linalg.matrix_power(A, d) for d in range(hop_size + 1)]\n",
    "    arrive_mat = (np.stack(transfer_mat) > 0)\n",
    "    for d in range(hop_size, -1, -1):\n",
    "        hop_dis[arrive_mat[d]] = d\n",
    "    return hop_dis\n",
    "\n",
    "  def normalize_digraph(self, A):\n",
    "    Dl = np.sum(A, 0)\n",
    "    num_node = A.shape[0]\n",
    "    Dn = np.zeros((num_node, num_node))\n",
    "    for i in range(num_node):\n",
    "        if Dl[i] > 0:\n",
    "            Dn[i, i] = Dl[i]**(-1)\n",
    "    DAD = np.dot(A, Dn)\n",
    "    return DAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGraphConvolution(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, s_kernel_size):\n",
    "    super().__init__()\n",
    "    self.s_kernel_size = s_kernel_size\n",
    "    self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=out_channels * s_kernel_size,\n",
    "                          kernel_size=1)\n",
    "    \n",
    "  def forward(self, x, A):\n",
    "    x = self.conv(x)\n",
    "    n, kc, t, v = x.size()\n",
    "    x = x.view(n, self.s_kernel_size, kc//self.s_kernel_size, t, v)\n",
    "    #인접 행렬에 graph convolution 진행 후 특징 더해줌 \n",
    "    x = torch.einsum('nkctv,kvw->nctw', (x, A))\n",
    "    return x.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#공간, 시간 그래프를 번갈아 수행하기 위해 \n",
    "\n",
    "class STGC_block(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, t_kernel_size, A_size, dropout=0.5):\n",
    "    super().__init__()\n",
    "    # 공간 그래프 convolution\n",
    "    self.sgc = SpatialGraphConvolution(in_channels=in_channels,\n",
    "                                       out_channels=out_channels,\n",
    "                                       s_kernel_size=A_size[0])\n",
    "    \n",
    "    # M: weight matirx -> 가장자리에 가중치 부여 \n",
    "    self.M = nn.Parameter(torch.ones(A_size))\n",
    "\n",
    "    self.tgc = nn.Sequential(nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Dropout(dropout),\n",
    "                            nn.Conv2d(out_channels,\n",
    "                                      out_channels,\n",
    "                                      (t_kernel_size, 1),\n",
    "                                      (stride, 1),\n",
    "                                      ((t_kernel_size - 1) // 2, 0)),\n",
    "                            nn.BatchNorm2d(out_channels),\n",
    "                            nn.ReLU())\n",
    "\n",
    "  def forward(self, x, A):\n",
    "    x = self.tgc(self.sgc(x, A * self.M))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#네트워크 모델\n",
    "class ST_GCN(nn.Module):\n",
    "  def __init__(self, num_classes, in_channels, t_kernel_size, hop_size):\n",
    "    super().__init__()\n",
    "    # 그래프 작성 \n",
    "    graph = Graph(hop_size)\n",
    "    A = torch.tensor(graph.A, dtype=torch.float32, requires_grad=False)\n",
    "    self.register_buffer('A', A)\n",
    "    A_size = A.size()\n",
    "  \n",
    "    # Batch Normalization\n",
    "    self.bn = nn.BatchNorm1d(in_channels * A_size[1])\n",
    "    \n",
    "    # STGC_blocks\n",
    "    self.stgc1 = STGC_block(in_channels, 32, 1, t_kernel_size, A_size)\n",
    "    self.stgc2 = STGC_block(32, 32, 1, t_kernel_size, A_size)\n",
    "    self.stgc3 = STGC_block(32, 32, 1, t_kernel_size, A_size)\n",
    "    self.stgc4 = STGC_block(32, 64, 2, t_kernel_size, A_size)\n",
    "    self.stgc5 = STGC_block(64, 64, 1, t_kernel_size, A_size)\n",
    "    self.stgc6 = STGC_block(64, 64, 1, t_kernel_size, A_size)\n",
    "\n",
    "    # Prediction\n",
    "    self.fc = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Batch Normalization\n",
    "    N, C, T, V = x.size() # batch, channel, frame, node\n",
    "    x = x.permute(0, 3, 1, 2).contiguous().view(N, V * C, T)\n",
    "    x = x.float().to(device)\n",
    "    x = self.bn(x)\n",
    "    x = x.view(N, V, C, T).permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "    # STGC_blocks\n",
    "    x = self.stgc1(x, self.A)\n",
    "    x = self.stgc2(x, self.A)\n",
    "    x = self.stgc3(x, self.A)\n",
    "    x = self.stgc4(x, self.A)\n",
    "    x = self.stgc5(x, self.A)\n",
    "    x = self.stgc6(x, self.A)\n",
    "\n",
    "    # Prediction\n",
    "    x = F.avg_pool2d(x, x.size()[2:])\n",
    "    x = x.view(N, -1, 1, 1)\n",
    "    x = self.fc(x)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.Size' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     38\u001b[0m output \u001b[39m=\u001b[39m model(data)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 39\u001b[0m \u001b[39mprint\u001b[39m(output\u001b[39m.\u001b[39;49mshape())\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(label\u001b[39m.\u001b[39mshape())\n\u001b[1;32m     42\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, label)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'torch.Size' object is not callable"
     ]
    }
   ],
   "source": [
    "#모델 학습\n",
    "NUM_EPOCH = 300\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "patience = 10\n",
    "\n",
    "# 모델 만들기\n",
    "model = ST_GCN(num_classes=13, \n",
    "                  in_channels=3,\n",
    "                  t_kernel_size=9, #시간 그래프 convolution 커널 크기(t_kernel_size x 1)\n",
    "                  hop_size=2).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# data 준비\n",
    "data_loader = dict()\n",
    "data_loader['train'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='animal_data/train_data.npy', label_path='data/train_label.npy'), batch_size=BATCH_SIZE, shuffle=True,)\n",
    "data_loader['val'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='animal_data/val_data.npy', label_path='data/val_label.npy'), batch_size=BATCH_SIZE, shuffle=False,)\n",
    "data_loader['test'] = torch.utils.data.DataLoader(dataset=Feeder(data_path='animal_data/test_data.npy', label_path='data/test_label.npy'), batch_size=BATCH_SIZE, shuffle=False,)\n",
    "\n",
    "# 학습 준비\n",
    "model.train()\n",
    "\n",
    "# 학습 진행 \n",
    "for epoch in range(1, NUM_EPOCH+1):\n",
    "  correct = 0\n",
    "  sum_loss = 0\n",
    "\n",
    "  for batch_idx, (data, label) in enumerate(data_loader['train']):\n",
    "    data = data.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    output = model(data).to(device)\n",
    "    print(output)\n",
    "    print(label)\n",
    "\n",
    "    loss = criterion(output, label)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    sum_loss += loss.item()\n",
    "    _, predict = torch.max(output.data, 1)\n",
    "    correct += (predict == label).sum().item()\n",
    "\n",
    "  print('# train; Epoch: {} | Loss: {:.4f} | Accuracy: {:.4f}'.format(epoch, sum_loss/len(data_loader['train'].dataset), (100. * correct / len(data_loader['train'].dataset))))\n",
    "\n",
    "  if epoch % 10 == 0:\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    sum_loss = 0\n",
    "\n",
    "    confusion_matrix = np.zeros((10, 10))\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (data, label) in enumerate(data_loader['val']):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output,label)\n",
    "        sum_loss += loss.item()\n",
    "\n",
    "        _, predict = torch.max(output.data, 1)\n",
    "        correct += (predict == label).sum().item()\n",
    "        val_loss = sum_loss/len(data_loader['val'].dataset)\n",
    "\n",
    "      print('# validation; Epoch: {} | Loss: {:.4f} | Accuracy: {:.4f}'.format(epoch, sum_loss/len(data_loader['val'].dataset), (100. * correct / len(data_loader['val'].dataset))))\n",
    "\n",
    "\n",
    "      if val_loss < best_valid_loss:\n",
    "        best_valid_loss = best_valid_loss\n",
    "        patience_counter = 0\n",
    "      else:\n",
    "        patience_counter +=1\n",
    "\n",
    "      if patience_counter >= patience:\n",
    "        print('Early stopping tirggered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "confusion_matrix = np.zeros((10, 10))\n",
    "with torch.no_grad():\n",
    "  for batch_idx, (data, label) in enumerate(data_loader['test']):\n",
    "    data = data.to(device)\n",
    "    label = label.to(device)\n",
    "\n",
    "    output = model(data)\n",
    "\n",
    "    _, predict = torch.max(output.data, 1)\n",
    "    correct += (predict == label).sum().item()\n",
    "\n",
    "    for l, p in zip(label.view(-1), predict.view(-1)):\n",
    "      confusion_matrix[l.long(), p.long()] += 1\n",
    "\n",
    "len_cm = len(confusion_matrix)\n",
    "for i in range(len_cm):\n",
    "    sum_cm = np.sum(confusion_matrix[i])\n",
    "    for j in range(len_cm):\n",
    "        confusion_matrix[i][j] = 100 * (confusion_matrix[i][j] / sum_cm)\n",
    "\n",
    "classes = ['drink', 'throw', 'sit down', 'stand up', 'clapping', 'hand waving', 'kicking', 'jump up', 'salute', 'falling down']\n",
    "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.tight_layout()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "plt.ylabel('True')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('# Test Accuracy: {:.3f}[%]'.format(100. * correct / len(data_loader['test'].dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
