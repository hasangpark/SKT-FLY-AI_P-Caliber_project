{"cells":[{"cell_type":"markdown","id":"81e8e69c-b236-4d01-9f78-0f705b8c915a","metadata":{"id":"81e8e69c-b236-4d01-9f78-0f705b8c915a"},"source":["<h1 style=\"font-size:30px;\">Fine-tuning YOLOv8 Pose Models for Animal Pose Estimation</h1>\n","\n","In this blog post, we will specifically deal with keypoints estimation of **dogs** and show you how to fine-tune the very popular **YOLOv8** pose models from Ultralytics.\n","\n","<img src = \"https://learnopencv.com/wp-content/uploads/2023/09/yolov8m-predictions.png\" width=1000>"]},{"cell_type":"markdown","id":"c97e0ca9-3848-458a-9dc1-84fd04e73706","metadata":{"id":"c97e0ca9-3848-458a-9dc1-84fd04e73706"},"source":["## Table of Contents\n","\n","* [1 The Stanford Dogs Dataset](#1-The-Stanford-Dogs-Dataset)\n","* [2 Download Image Data and Keypoint Metadata](#2-Download-Image-Data-and-Keypoint-Metadata)\n","* [3 Create YOLO Train and Valid Directories](#3-Create-YOLO-Train-and-Valid-Directories)\n","* [4 Data Visualization](#4-Data-Visualization)\n","* [5 Configurations](#5-Configurations)\n","* [6 Training](#6-Training)\n","* [7 Evaluation](#7-Evaluation)\n","* [8 Predictions](#8-Predictions)"]},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lEITwUsIyedW","executionInfo":{"status":"ok","timestamp":1696860933079,"user_tz":-540,"elapsed":8102,"user":{"displayName":"박상하","userId":"01175582064356018376"}},"outputId":"c9892bc0-a836-4793-d701-c3cf45aa563f"},"id":"lEITwUsIyedW","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.0.195-py3-none-any.whl (618 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m618.9/618.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: numpy>=1.22.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.23.5)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.1+cu118)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.15.2+cu118)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.1)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.12.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Collecting thop>=0.1.1 (from ultralytics)\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.43.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (3.27.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8.0->ultralytics) (17.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Installing collected packages: thop, ultralytics\n","Successfully installed thop-0.1.1.post2209072238 ultralytics-8.0.195\n"]}]},{"cell_type":"code","execution_count":null,"id":"f2d691f3-1431-486a-aed7-fead57df9a3c","metadata":{"id":"f2d691f3-1431-486a-aed7-fead57df9a3c"},"outputs":[],"source":["import os\n","import time\n","import json\n","import requests\n","from zipfile import ZipFile\n","import tarfile\n","from shutil import copyfile\n","from dataclasses import dataclass, field\n","\n","import yaml\n","import glob\n","\n","import random\n","import numpy as np\n","import pandas as pd\n","import cv2\n","\n","from ultralytics import YOLO\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"rQ2qAceTyh67"},"id":"rQ2qAceTyh67","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"8fe8e3b8-a563-4c0b-a6bc-e9fde858b9f1","metadata":{"id":"8fe8e3b8-a563-4c0b-a6bc-e9fde858b9f1"},"source":["## 1 The Stanford Dogs Dataset\n","\n","For our experiments, we will use the **Stanford Dataset**, which contains 120 breeds of dogs across **20,580** images. Besides, the dataset also contains the bounding box annotations for these images.\n","\n","However, the keypoint annotations need to be downloaded from the **StandfordExtra** dataset by filling up a **[google form](https://forms.gle/sRtbicgxsWvRtRmUA)**. The keypoint annotations are provided across **12,538** images for `20` keypoints of dog pose (`3` for each leg, `2` for each ear, `2` for the tail, nose, and jaw).\n","\n","<img src=\"https://learnopencv.com/wp-content/uploads/2023/09/animal-pose-estimation-dog-kpts.png\" width=700>\n","\n","The authors have also provided keypoint metadata in the form of a CSV file containing the animal pose name, the color coding for each keypoint, etc. It, however, contains the info across 2"]},{"cell_type":"markdown","id":"2c55a89f-5d0f-412a-a46d-719ad4e9a485","metadata":{"id":"2c55a89f-5d0f-412a-a46d-719ad4e9a485"},"source":["## 2 Download Image Data and Keypoint Metadata"]},{"cell_type":"markdown","id":"66c4e95c-d726-4122-a328-243aacccb6c2","metadata":{"id":"66c4e95c-d726-4122-a328-243aacccb6c2"},"source":["The `download_and_unzip` utility downloads and extracts the **`images.tar`** file containing the images. Besides, we shall also download the **`keypoint_definitions.csv`** containing the keypoint metadata, such as the animal pose name, color coding for each keypoint, etc., across all 24  keypoints."]},{"cell_type":"code","execution_count":1,"id":"30414c02-3d41-4d1b-a8b4-5b98a8da6cce","metadata":{"id":"30414c02-3d41-4d1b-a8b4-5b98a8da6cce","executionInfo":{"status":"ok","timestamp":1696908008374,"user_tz":-540,"elapsed":505,"user":{"displayName":"박상하","userId":"01175582064356018376"}}},"outputs":[],"source":["# Download and dataset.\n","def download_and_unzip(url, save_path):\n","\n","    print(\"Downloading and extracting assets...\", end=\"\")\n","    file = requests.get(url)\n","    open(save_path, \"wb\").write(file.content)\n","\n","    try:\n","        # Extract tarfile.\n","        if save_path.endswith(\".tar\"):\n","            with tarfile.open(save_path, \"r\") as tar:\n","                tar.extractall(os.path.split(save_path)[0])\n","\n","        print(\"Done\")\n","    except:\n","        print(\"Invalid file\")"]},{"cell_type":"markdown","id":"0c43929d-4d80-4c45-a7be-4a6685e99f3f","metadata":{"id":"0c43929d-4d80-4c45-a7be-4a6685e99f3f"},"source":["All the downloaded images are extracted to the Images  directory. It has the following directory structure:\n","\n","```python\n","Images/\n","├── n02085620-Chihuahua\n","│   ├── n02085620_10074.jpg\n","│   ├── n02085620_10131.jpg\n","│   └── ...\n","├── n02085782-Japanese_spaniel\n","│   ├── n02085782_1039.jpg\n","│   ├── n02085782_1058.jpg\n","│   └── n02085782_962.jpg\n","└── ...\n","```"]},{"cell_type":"code","execution_count":2,"id":"c0ed3126-089f-44ab-8724-59bf86108480","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0ed3126-089f-44ab-8724-59bf86108480","executionInfo":{"status":"error","timestamp":1696908008818,"user_tz":-540,"elapsed":9,"user":{"displayName":"박상하","userId":"01175582064356018376"}},"outputId":"96f53dda-52ed-425a-bdd6-5fe9f8494dc7"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-35053027403c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mIMAGES_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mIMAGES_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Images\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mIMAGES_TAR_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{IMAGES_DIR}.tar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mANNS_METADATA_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}],"source":["IMAGES_URL = r\"http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\"\n","IMAGES_DIR = \"Images\"\n","IMAGES_TAR_PATH = os.path.join(os.getcwd(), f\"{IMAGES_DIR}.tar\")\n","\n","ANNS_METADATA_URL = r\"https://github.com/benjiebob/StanfordExtra/raw/master/keypoint_definitions.csv\"\n","ANNS_METADATA = \"keypoint_definitions.csv\"\n","\n","# Download if dataset does not exists.\n","if not os.path.exists(IMAGES_DIR):\n","    download_and_unzip(IMAGES_URL, IMAGES_TAR_PATH)\n","    os.remove(IMAGES_TAR_PATH)\n","\n","if not os.path.isfile(ANNS_METADATA):\n","    download_and_unzip(ANNS_METADATA_URL, ANNS_METADATA)"]},{"cell_type":"markdown","id":"1dd54ce2-5830-4c21-8b61-65f27d5742e7","metadata":{"id":"1dd54ce2-5830-4c21-8b61-65f27d5742e7"},"source":["The annotations downloaded after filling out the form mentioned earlier are maintained in the `StanfordExtra_V12` directory which contains the annotation **JSON** file: `StanfordExtra_v12.json` contain the following structure:\n","\n","```python\n","StanfordExtra_V12\n","├── StanfordExtra_v12.json\n","├── test_stanford_StanfordExtra_v12.npy\n","├── train_stanford_StanfordExtra_v12.npy\n","└── val_stanford_StanfordExtra_v12.npy\n","```\n","\n","The train, validation, and test splits are provided as indices from the original **`StanfordExtra_v12.json`** data.\n","\n","The train, validation, and test sets contain annotations for **6773**, **4062**, and **1703** images, respectively."]},{"cell_type":"code","execution_count":null,"id":"7177139c-ed62-48db-8f07-583da0546f72","metadata":{"id":"7177139c-ed62-48db-8f07-583da0546f72","executionInfo":{"status":"aborted","timestamp":1696908008819,"user_tz":-540,"elapsed":7,"user":{"displayName":"박상하","userId":"01175582064356018376"}}},"outputs":[],"source":["ANN_PATH = \"/content/drive/MyDrive/StanfordExtra_V12\"\n","JSON_PATH = os.path.join(ANN_PATH, \"StanfordExtra_v12.json\")\n","\n","with open(JSON_PATH) as file:\n","    json_data = json.load(file)"]},{"cell_type":"markdown","id":"f5258f50-139a-40cf-8172-0a20a0ad7f3d","metadata":{"id":"f5258f50-139a-40cf-8172-0a20a0ad7f3d"},"source":["The files: **`train_stanford_StanfordExtra_v12.npy`** and **`test_stanford_StanfordExtra_v12.npy`** consist of the training and validation indices with respect to the original json_data list.\n","\n","For simplicity, we shall use the test data for validation. The training and the test sets comprise **6773** and **1703** samples, respectively."]},{"cell_type":"code","execution_count":null,"id":"a332c0b6-e228-44f9-878b-081f28d822dd","metadata":{"id":"a332c0b6-e228-44f9-878b-081f28d822dd","executionInfo":{"status":"aborted","timestamp":1696908008819,"user_tz":-540,"elapsed":6,"user":{"displayName":"박상하","userId":"01175582064356018376"}}},"outputs":[],"source":["\n","\n","\n","train_ids = np.load(os.path.join(ANN_PATH,\n","                                 \"train_stanford_StanfordExtra_v12.npy\"))\n","val_ids = np.load(os.path.join(ANN_PATH,\n","                               \"test_stanford_StanfordExtra_v12.npy\"))\n","\n","print(f\"Train Samples: {len(train_ids)}\")\n","print(f\"Validation Samples: {len(val_ids)}\")"]},{"cell_type":"markdown","id":"ce89f50b-e951-40ab-9f44-68e68f731347","metadata":{"id":"ce89f50b-e951-40ab-9f44-68e68f731347"},"source":["## 3 Create YOLO Train and Valid Directories"]},{"cell_type":"markdown","id":"012e6114-d72d-44f9-8da5-57fde95db878","metadata":{"id":"012e6114-d72d-44f9-8da5-57fde95db878"},"source":["We will maintain the following directory structure for YOLOv8 dataset:\n","\n","```python\n","animal-pose-data\n","├── train\n","│   ├── images (6773 files)\n","│   └── labels (6773 files)\n","└── valid\n","    ├── images (1703 files)\n","    └── labels (1703 files)\n","```"]},{"cell_type":"code","execution_count":null,"id":"fc48c66c-4fb4-45a4-b150-a4e5d5dfcc1c","metadata":{"id":"fc48c66c-4fb4-45a4-b150-a4e5d5dfcc1c","executionInfo":{"status":"aborted","timestamp":1696908008819,"user_tz":-540,"elapsed":6,"user":{"displayName":"박상하","userId":"01175582064356018376"}}},"outputs":[],"source":["DATA_DIR = \"animal-pose-data\"\n","\n","TRAIN_DIR         = f\"train\"\n","TRAIN_FOLDER_IMG    = f\"images\"\n","TRAIN_FOLDER_LABELS = f\"labels\"\n","\n","TRAIN_IMG_PATH   = os.path.join(DATA_DIR, TRAIN_DIR, TRAIN_FOLDER_IMG)\n","TRAIN_LABEL_PATH = os.path.join(DATA_DIR, TRAIN_DIR, TRAIN_FOLDER_LABELS)\n","\n","VALID_DIR           = f\"valid\"\n","VALID_FOLDER_IMG    = f\"images\"\n","VALID_FOLDER_LABELS = f\"labels\"\n","\n","VALID_IMG_PATH   = os.path.join(DATA_DIR, VALID_DIR, VALID_FOLDER_IMG)\n","VALID_LABEL_PATH = os.path.join(DATA_DIR, VALID_DIR, VALID_FOLDER_LABELS)\n","\n","os.makedirs(TRAIN_IMG_PATH, exist_ok=True)\n","os.makedirs(TRAIN_LABEL_PATH, exist_ok=True)\n","os.makedirs(VALID_IMG_PATH, exist_ok=True)\n","os.makedirs(VALID_LABEL_PATH, exist_ok=True)"]},{"cell_type":"markdown","id":"3107901d-b191-4e67-b7a2-f89cd7b13a02","metadata":{"id":"3107901d-b191-4e67-b7a2-f89cd7b13a02"},"source":["Next, we will use `train_ids` and `val_ids` to gather the image and annotation data using `json_data` obtained earlier."]},{"cell_type":"code","execution_count":null,"id":"3786dc4a-13bd-407d-9948-317c897068c8","metadata":{"id":"3786dc4a-13bd-407d-9948-317c897068c8"},"outputs":[],"source":["train_json_data = []\n","for train_id in train_ids:\n","    train_json_data.append(json_data[train_id])\n","\n","val_json_data = []\n","for val_id in val_ids:\n","    val_json_data.append(json_data[val_id])"]},{"cell_type":"markdown","id":"883fbe67-9a16-4fed-bcad-80289bb83d2c","metadata":{"id":"883fbe67-9a16-4fed-bcad-80289bb83d2c"},"source":["### 3.1 Copy Image files"]},{"cell_type":"code","execution_count":null,"id":"c82fd4e1-58a1-4af0-90f3-4392025356a7","metadata":{"id":"c82fd4e1-58a1-4af0-90f3-4392025356a7"},"outputs":[],"source":["for data in train_json_data:\n","    img_file = data[\"img_path\"]\n","    filename = img_file.split(\"/\")[-1]\n","    copyfile(os.path.join(IMAGES_DIR, img_file),\n","             os.path.join(TRAIN_IMG_PATH, filename))\n","\n","\n","for data in val_json_data:\n","    img_file = data[\"img_path\"]\n","    filename = img_file.split(\"/\")[-1]\n","    copyfile(os.path.join(IMAGES_DIR, img_file),\n","             os.path.join(VALID_IMG_PATH, filename))"]},{"cell_type":"markdown","id":"3dc611e8-9105-4392-823c-4ff12256f5e4","metadata":{"id":"3dc611e8-9105-4392-823c-4ff12256f5e4"},"source":["### 3.2 Create YOLO Annotation TXT FILES"]},{"cell_type":"markdown","id":"1d036a60-0f0e-4652-8f5d-fd7d101a9f76","metadata":{"id":"1d036a60-0f0e-4652-8f5d-fd7d101a9f76"},"source":["Our final task for data preparation is to create the boxes and the keypoint annotations in accordance with Ultralytics’ YOLO. Since we will deal with a single class (i.e., dogs), we set the class index to **`0`**."]},{"cell_type":"code","execution_count":null,"id":"b11d75bd-e340-4696-9d9b-97381ae1fd25","metadata":{"id":"b11d75bd-e340-4696-9d9b-97381ae1fd25"},"outputs":[],"source":["CLASS_ID = 0"]},{"cell_type":"markdown","id":"4ef7d2a3-8c74-44f9-ae28-b1b3401b7aec","metadata":{"id":"4ef7d2a3-8c74-44f9-ae28-b1b3401b7aec"},"source":["The function **`create_yolo_boxes_kpts`** performs the following tasks:\n","\n","* Modifies visibility indicators for keypoints (setting the visibilities for labeled keypoints to 2).\n","* Normalizes the coordinates of both bounding boxes and keypoints relative to the image dimensions.\n","* Converts bounding boxes to $[x_{center}, \\ , y_{center},\\ width,\\ height]$ in normalized form."]},{"cell_type":"code","execution_count":null,"id":"c2871243-41d0-4a36-92c1-17d4f250fbcf","metadata":{"id":"c2871243-41d0-4a36-92c1-17d4f250fbcf"},"outputs":[],"source":["def create_yolo_boxes_kpts(img_size, boxes, lm_kpts):\n","\n","    IMG_W, IMG_H = img_size\n","    # Modify kpts with visibilities as 1s to 2s.\n","    vis_ones = np.where(lm_kpts[:, -1] == 1.)\n","    lm_kpts[vis_ones, -1] = 2.\n","\n","    # Normalizing factor for bboxes and kpts.\n","    res_box_array = np.array([IMG_W, IMG_H, IMG_W, IMG_H])\n","    res_lm_array = np.array([IMG_W, IMG_H])\n","\n","    # Normalize landmarks in the range [0,1].\n","    norm_kps_per_img = lm_kpts.copy()\n","    norm_kps_per_img[:, :-1]  = norm_kps_per_img[:, :-1] / res_lm_array\n","\n","    # Normalize bboxes in the range [0,1].\n","    norm_bbox_per_img = boxes / res_box_array\n","\n","    # Create bboxes coordinates to YOLO.\n","    # x_c, y_c = x_min + bbox_w/2. , y_min + bbox_h/2.\n","    yolo_boxes = norm_bbox_per_img.copy()\n","    yolo_boxes[:2] = norm_bbox_per_img[:2] + norm_bbox_per_img[2:]/2.\n","\n","    return yolo_boxes, norm_kps_per_img"]},{"cell_type":"markdown","id":"fda4abe0-3467-4575-97d6-51e4c1226b73","metadata":{"id":"fda4abe0-3467-4575-97d6-51e4c1226b73"},"source":["We will finally create the `txt` files for YOLO based on the `train_json_data` and `val_json_data` obtained earlier. The function `create_yolo_txt_files` creates the required `txt` annotations in YOLO using the `create_yolo_boxes_kpts` utility function explained above."]},{"cell_type":"code","execution_count":null,"id":"6f7fa0ee-d9df-4d4f-8493-012a39959bcb","metadata":{"id":"6f7fa0ee-d9df-4d4f-8493-012a39959bcb"},"outputs":[],"source":["def create_yolo_txt_files(json_data, LABEL_PATH):\n","\n","    for data in json_data:\n","\n","        IMAGE_ID = data[\"img_path\"].split(\"/\")[-1].split(\".\")[0]\n","\n","        IMG_WIDTH, IMG_HEIGHT = data[\"img_width\"], data[\"img_height\"]\n","\n","        landmark_kpts  = np.nan_to_num(np.array(data[\"joints\"], dtype=np.float32))\n","        landmarks_bboxes = np.array(data[\"img_bbox\"], dtype=np.float32)\n","\n","        bboxes_yolo, kpts_yolo = create_yolo_boxes_kpts(\n","                                            (IMG_WIDTH, IMG_HEIGHT),\n","                                            landmarks_bboxes,\n","                                            landmark_kpts)\n","\n","        TXT_FILE = IMAGE_ID+\".txt\"\n","\n","        with open(os.path.join(LABEL_PATH, TXT_FILE), \"w\") as f:\n","\n","            x_c_norm, y_c_norm, box_width_norm, box_height_norm = round(bboxes_yolo[0],5),\\\n","                                                                  round(bboxes_yolo[1],5),\\\n","                                                                  round(bboxes_yolo[2],5),\\\n","                                                                  round(bboxes_yolo[3],5),\\\n","\n","            kps_flattend = [round(ele,5) for ele in kpts_yolo.flatten().tolist()]\n","            line = f\"{CLASS_ID} {x_c_norm} {y_c_norm} {box_width_norm} {box_height_norm} \"\n","            line+= \" \".join(map(str, kps_flattend))\n","            f.write(line)"]},{"cell_type":"markdown","id":"9bbb9989-e536-45d5-b198-771a16ce3025","metadata":{"id":"9bbb9989-e536-45d5-b198-771a16ce3025"},"source":["Finnally, we create the train and validation data."]},{"cell_type":"code","execution_count":null,"id":"a0c6a84a-5678-475d-b95a-3b2b6ccba51c","metadata":{"id":"a0c6a84a-5678-475d-b95a-3b2b6ccba51c"},"outputs":[],"source":["create_yolo_txt_files(train_json_data, TRAIN_LABEL_PATH)\n","create_yolo_txt_files(val_json_data, VALID_LABEL_PATH)"]},{"cell_type":"markdown","id":"aa98afac-c5cf-4347-985c-6286362316dd","metadata":{"id":"aa98afac-c5cf-4347-985c-6286362316dd"},"source":["## 4 Data Visualization"]},{"cell_type":"markdown","id":"010f3b5e-5554-4309-8992-0ef691aa93ee","metadata":{"id":"010f3b5e-5554-4309-8992-0ef691aa93ee"},"source":["Before visualizing the samples, we can map the `hexadecimal` color codings available with **`keypoint_definitions.csv`** to RGB values."]},{"cell_type":"code","execution_count":null,"id":"feb68085-e8ac-49d2-a319-83f6d0fad95a","metadata":{"id":"feb68085-e8ac-49d2-a319-83f6d0fad95a"},"outputs":[],"source":["ann_meta_data = pd.read_csv(\"keypoint_definitions.csv\")\n","COLORS = ann_meta_data[\"Hex colour\"].values.tolist()\n","\n","COLORS_RGB_MAP = []\n","for color in COLORS:\n","    R, G, B = int(color[:2], 16), int(color[2:4], 16), int(color[4:], 16)\n","    COLORS_RGB_MAP.append({color: (R,G,B)})"]},{"cell_type":"code","execution_count":null,"id":"605d8a97-a77b-4e37-a51f-dff797ec73ab","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"605d8a97-a77b-4e37-a51f-dff797ec73ab","executionInfo":{"status":"ok","timestamp":1696861002115,"user_tz":-540,"elapsed":3,"user":{"displayName":"박상하","userId":"01175582064356018376"}},"outputId":"1d512feb-fb06-47c0-c75b-bc61a0431a8a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training images: 6773, Validation Images: 1703\n"]}],"source":["train_images = os.listdir(TRAIN_IMG_PATH)\n","valid_images = os.listdir(VALID_IMG_PATH)\n","\n","print(f\"Training images: {len(train_images)}, Validation Images: {len(valid_images)}\")"]},{"cell_type":"markdown","id":"78b31f59-c466-42a7-87a6-68a0c9a0c82f","metadata":{"id":"78b31f59-c466-42a7-87a6-68a0c9a0c82f"},"source":["The `draw_landmarks` function is used to annotate the corresponding landmark points on the image using COLORS_RGB_MAP."]},{"cell_type":"code","execution_count":null,"id":"f99e7df6-4a58-4e85-ab41-ebd533278f8e","metadata":{"id":"f99e7df6-4a58-4e85-ab41-ebd533278f8e"},"outputs":[],"source":["def draw_landmarks(image, landmarks):\n","\n","    radius = 5\n","    # Check if image width is greater than 1000 px.\n","    # To improve visualization.\n","    if (image.shape[1] > 1000):\n","        radius = 8\n","\n","    for idx, kpt_data in enumerate(landmarks):\n","\n","        loc_x, loc_y = kpt_data[:2].astype(\"int\").tolist()\n","        color_id = list(COLORS_RGB_MAP[int(kpt_data[-1])].values())[0]\n","\n","        cv2.circle(image,\n","                   (loc_x, loc_y),\n","                   radius,\n","                   color=color_id[::-1],\n","                   thickness=-1,\n","                   lineType=cv2.LINE_AA)\n","\n","    return image"]},{"cell_type":"markdown","id":"e4535983-422b-480e-bb3d-2a79dd028eca","metadata":{"id":"e4535983-422b-480e-bb3d-2a79dd028eca"},"source":["The `draw_boxes` function is used to annotate the bounding boxes along with the confidence scores (if passed) on the image."]},{"cell_type":"code","execution_count":null,"id":"7cb17095-84bb-499e-aea4-4fc9c4e67d0f","metadata":{"id":"7cb17095-84bb-499e-aea4-4fc9c4e67d0f"},"outputs":[],"source":["def draw_boxes(image, detections, class_name = \"dog\", score=None, color=(0,255,0)):\n","\n","    font_size = 0.25 + 0.07 * min(image.shape[:2]) / 100\n","    font_size = max(font_size, 0.5)\n","    font_size = min(font_size, 0.8)\n","    text_offset = 3\n","\n","    thickness = 2\n","    # Check if image width is greater than 1000 px.\n","    # To improve visualization.\n","    if (image.shape[1] > 1000):\n","        thickness = 10\n","\n","    xmin, ymin, xmax, ymax = detections[:4].astype(\"int\").tolist()\n","    conf = round(float(detections[-1]),2)\n","    cv2.rectangle(image,\n","                  (xmin, ymin),\n","                  (xmax, ymax),\n","                  color=(0,255,0),\n","                  thickness=thickness,\n","                  lineType=cv2.LINE_AA)\n","\n","    display_text = f\"{class_name}\"\n","\n","    if score is not None:\n","        display_text+=f\": {score:.2f}\"\n","\n","    (text_width, text_height), _ = cv2.getTextSize(display_text,\n","                                                   cv2.FONT_HERSHEY_SIMPLEX,\n","                                                   font_size, 2)\n","\n","    cv2.rectangle(image,\n","                      (xmin, ymin),\n","                      (xmin + text_width + text_offset, ymin - text_height - int(15 * font_size)),\n","                      color=color, thickness=-1)\n","\n","    image = cv2.putText(\n","                    image,\n","                    display_text,\n","                    (xmin + text_offset, ymin - int(10 * font_size)),\n","                    cv2.FONT_HERSHEY_SIMPLEX,\n","                    font_size,\n","                    (0, 0, 0),\n","                    2, lineType=cv2.LINE_AA,\n","                )\n","\n","    return image"]},{"cell_type":"markdown","id":"283a9245-e884-4dec-b667-168717117b9e","metadata":{"id":"283a9245-e884-4dec-b667-168717117b9e"},"source":["The `visualize_annotations` is used to annotate both the bounding box coordinates and the landmark keypoints on the corresponding image after converting them to absoulute coordinates.\n","\n","Recall that both the bounding box coordinates and the keypoints were normalized in the range `[0, 1]`. However, to plot them, we need the absolute coordinates.\n","\n","The conversion mapping from YOLO bboxes to $[x_{min}, y_{min}, x_{max}, y_{max}]$ is pretty straight forward and can be obtained using the following set of equations:\n","\n","$$x_{min} = \\frac{W}{2} (2x_{center} \\ - \\ width)$$\n","\n","$$y_{min} = \\frac{H}{2} (2y_{center} \\ - \\ height)$$\n","\n","$$x_{max} = x_{min} + width * W$$\n","\n","$$y_{max} = y_{min} + height * H$$\n","\n","\n","Similarly, the keypoints can denormalized (to the absolute coordinates) using:\n","\n","$$x_{abs} = x_{norm}* W$$\n","\n","$$y_{abs} = y_{norm}* H$$\n","\n","\n","Here, the `width` and `height` are the box width and height respectively; whereas `W` and `H` are the image width and height respectively."]},{"cell_type":"code","execution_count":null,"id":"d2a22ae9-59bc-468e-8217-0c5cc67a75dc","metadata":{"id":"d2a22ae9-59bc-468e-8217-0c5cc67a75dc"},"outputs":[],"source":["def visualize_annotations(image, box_data, keypoints_data):\n","\n","    image = image.copy()\n","\n","    shape_multiplier = np.array(image.shape[:2][::-1]) # (W, H).\n","    # Final absolute coordinates (xmin, ymin, xmax, ymax).\n","    denorm_boxes = np.zeros_like(box_data)\n","\n","    # De-normalize center coordinates from YOLO to (xmin, ymin).\n","    denorm_boxes[:, :2] = (shape_multiplier/2.) * (2*box_data[:,:2] - box_data[:,2:])\n","\n","    # De-normalize width and height from YOLO to (xmax, ymax).\n","    denorm_boxes[:, 2:] = denorm_boxes[:,:2] + box_data[:,2:]*shape_multiplier\n","\n","    for boxes, kpts in zip(denorm_boxes, keypoints_data):\n","        # De-normalize landmark coordinates.\n","        kpts[:, :2]*= shape_multiplier\n","        image = draw_boxes(image, boxes)\n","        image = draw_landmarks(image, kpts)\n","\n","    return image"]},{"cell_type":"markdown","id":"b084158f-5e58-419b-8cfc-40fab86839ae","metadata":{"id":"b084158f-5e58-419b-8cfc-40fab86839ae"},"source":["The following plot shows a few image samples with their corresponding ground truth annotation. The keypoint annotations are filtered based on their corresponding visibility flag."]},{"cell_type":"code","execution_count":null,"id":"326aa95f-33d7-43c3-868a-826324258898","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":625,"output_embedded_package_id":"1qN3ZKWkoHgXMWjlv_EqOSikJctziVsvc"},"id":"326aa95f-33d7-43c3-868a-826324258898","executionInfo":{"status":"ok","timestamp":1696861008204,"user_tz":-540,"elapsed":6091,"user":{"displayName":"박상하","userId":"01175582064356018376"}},"outputId":"194958cc-3c13-4fd1-e689-f80f4c00bce2"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["IMAGE_FILES = os.listdir(TRAIN_IMG_PATH)\n","NUM_LANDMARKS = 24\n","\n","num_samples = 8\n","num_rows = 2\n","num_cols = num_samples//num_rows\n","\n","fig, ax = plt.subplots(\n","        nrows=num_rows,\n","        ncols=num_cols,\n","        figsize=(25, 15),\n","    )\n","\n","random.seed(45)\n","random.shuffle(IMAGE_FILES)\n","\n","for idx, (file, axis) in enumerate(zip(IMAGE_FILES[:num_samples], ax.flat)):\n","\n","    image = cv2.imread(os.path.join(TRAIN_IMG_PATH, file))\n","\n","    # Obtain the txt file for the corresponding image file.\n","    filename = file.split(\".\")[0]\n","    # Split each object instance in separate lists.\n","    with open(os.path.join(TRAIN_LABEL_PATH, filename+\".txt\"), \"r\") as file:\n","        label_data = [x.split() for x in file.read().strip().splitlines() if len(x)]\n","\n","    label_data = np.array(label_data, dtype=np.float32)\n","\n","    # YOLO BBox instances in [x-center, y-center, width, height] in normalized form.\n","    box_instances = label_data[:,1:5]\n","    # Shape: (N, 4), where, N = #instances per-image\n","\n","    # Kpt instances.\n","    # Filter keypoints based on visibility.\n","    instance_kpts = []\n","    kpts_data = label_data[:,5:].reshape(-1, NUM_LANDMARKS, 3)\n","\n","    for inst_kpt in kpts_data:\n","        vis_ids = np.where(inst_kpt[:, -1]>0.)[0]\n","        vis_kpts = inst_kpt[vis_ids][:,:2]\n","        vis_kpts = np.concatenate([vis_kpts, np.expand_dims(vis_ids, axis=-1)], axis=-1)\n","        instance_kpts.append(vis_kpts)\n","\n","    image_ann = visualize_annotations(image, box_instances, instance_kpts)\n","    axis.imshow(image_ann[...,::-1])\n","    axis.axis(\"off\")\n","\n","\n","plt.tight_layout(h_pad=4., w_pad=4.)\n","plt.show();"]},{"cell_type":"markdown","id":"98a171a6-8a1d-4215-bbdf-a0c0bdde5c36","metadata":{"id":"98a171a6-8a1d-4215-bbdf-a0c0bdde5c36"},"source":["## 5 Configurations"]},{"cell_type":"markdown","id":"e5071680-68cc-4240-9ef5-e6fc609ba156","metadata":{"id":"e5071680-68cc-4240-9ef5-e6fc609ba156"},"source":["### 5.1 Training Configuration\n","\n","We shall define the training configuration for fine-tuning in the `TrainingConfig` class."]},{"cell_type":"code","execution_count":null,"id":"7e754d5c-1be0-4238-b113-3a02ad8f25a6","metadata":{"id":"7e754d5c-1be0-4238-b113-3a02ad8f25a6"},"outputs":[],"source":["@dataclass(frozen=True)\n","class TrainingConfig:\n","    DATASET_YAML:   str = \"animal-keypoints.yaml\"\n","    MODEL:          str = \"yolov8m-pose.pt\"\n","    EPOCHS:         int = 100\n","    KPT_SHAPE:    tuple = (24,3)\n","    PROJECT:        str = \"Animal_Keypoints\"\n","    NAME:           str = f\"{MODEL.split('.')[0]}_{EPOCHS}_epochs\"\n","    CLASSES_DICT:  dict = field(default_factory = lambda:{0 : \"dog\"})"]},{"cell_type":"markdown","id":"120c05db-5e44-4574-9a23-e8fc00e7b099","metadata":{"id":"120c05db-5e44-4574-9a23-e8fc00e7b099"},"source":["### 5.2 Data Configuration"]},{"cell_type":"markdown","id":"3473ee95-d50d-4cf3-b2de-e63d75b03895","metadata":{"id":"3473ee95-d50d-4cf3-b2de-e63d75b03895"},"source":["The `DatasetConfig` class takes in the various hyperparameters related to the data such as the image size and batch size to be used while training, along with the various augmentation probabilities such as Mosaic, horizontal flip, etc."]},{"cell_type":"code","execution_count":null,"id":"1769ab0c-7773-4fe4-a828-76ec17d3cff8","metadata":{"id":"1769ab0c-7773-4fe4-a828-76ec17d3cff8"},"outputs":[],"source":["@dataclass(frozen=True)\n","class DatasetConfig:\n","    IMAGE_SIZE:    int   = 640\n","    BATCH_SIZE:    int   = 16\n","    CLOSE_MOSAIC:  int   = 10\n","    MOSAIC:        float = 0.4\n","    FLIP_LR:       float = 0.0 # Turn off horizontal flip."]},{"cell_type":"code","execution_count":null,"id":"005212b5-80e9-4487-9f1f-3c9ec2de6fcc","metadata":{"id":"005212b5-80e9-4487-9f1f-3c9ec2de6fcc"},"outputs":[],"source":["train_config = TrainingConfig()\n","data_config = DatasetConfig()"]},{"cell_type":"markdown","id":"be43529c-85a0-4bb7-86e4-463bfab82d54","metadata":{"id":"be43529c-85a0-4bb7-86e4-463bfab82d54"},"source":["Before we start our training, we need to create a `yaml` containing the path to the images and label files. We also need to specify the class names, starting from index=0 and the keypoint shape."]},{"cell_type":"code","execution_count":null,"id":"77c63d59-91b9-455a-92cc-355af5aa0dc5","metadata":{"id":"77c63d59-91b9-455a-92cc-355af5aa0dc5"},"outputs":[],"source":["current_dir = os.getcwd()\n","\n","data_dict = dict(\n","                path      = os.path.join(current_dir, DATA_DIR),\n","                train     = os.path.join(TRAIN_DIR, TRAIN_FOLDER_IMG),\n","                val       = os.path.join(VALID_DIR, VALID_FOLDER_IMG),\n","                names     = train_config.CLASSES_DICT,\n","                kpt_shape = list(train_config.KPT_SHAPE),\n","               )\n","\n","with open(train_config.DATASET_YAML, \"w\") as config_file:\n","    yaml.dump(data_dict, config_file)pose_model = model = YOLO(train_config.MODEL)\n","\n","pose_model.train(data    = train_config.DATASET_YAML,\n","            epochs       = train_config.EPOCHS,\n","            imgsz        = data_config.IMAGE_SIZE,\n","            batch        = data_config.BATCH_SIZE,\n","            project      = train_config.PROJECT,\n","            name         = train_config.NAME,\n","            close_mosaic = data_config.CLOSE_MOSAIC,\n","            mosaic       = data_config.MOSAIC,\n","            fliplr       = data_config.FLIP_LR\n","           )"]},{"cell_type":"markdown","id":"13a7c435-78fb-4d76-aea0-335da96db326","metadata":{"id":"13a7c435-78fb-4d76-aea0-335da96db326"},"source":["## 6 Training"]},{"cell_type":"code","execution_count":null,"id":"54fffd85-cccc-446e-b51a-b885991a2b6f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"54fffd85-cccc-446e-b51a-b885991a2b6f","outputId":"9aae89f4-a4d9-4fa5-e8d8-50f61ab5163a"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-pose.pt to 'yolov8m-pose.pt'...\n","100%|██████████| 50.8M/50.8M [00:00<00:00, 147MB/s]\n","Ultralytics YOLOv8.0.195 🚀 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=pose, mode=train, model=yolov8m-pose.pt, data=animal-keypoints.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=Animal_Keypoints, name=yolov8m-pose_100_epochs, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.0, mosaic=0.4, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=Animal_Keypoints/yolov8m-pose_100_epochs\n","Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","100%|██████████| 755k/755k [00:00<00:00, 15.4MB/s]\n","Overriding model.yaml kpt_shape=[17, 3] with kpt_shape=[24, 3]\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n","  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n","  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n","  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n","  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n","  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n","  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n","  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n","  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n","  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n"," 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n"," 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n"," 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n"," 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n"," 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n"," 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n"," 22        [15, 18, 21]  1   4679371  ultralytics.nn.modules.head.Pose             [1, [24, 3], [192, 384, 576]] \n","YOLOv8m-pose summary: 320 layers, 26759995 parameters, 26759979 gradients, 82.6 GFLOPs\n","\n","Transferred 481/517 items from pretrained weights\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir Animal_Keypoints/yolov8m-pose_100_epochs', view at http://localhost:6006/\n","Freezing layer 'model.22.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n","Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n","100%|██████████| 6.23M/6.23M [00:00<00:00, 61.8MB/s]\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/animal-pose-data/train/labels... 6773 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6773/6773 [00:06<00:00, 1043.71it/s]\n","\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/animal-pose-data/train/images/n02089973_2054.jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/animal-pose-data/train/labels.cache\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/animal-pose-data/valid/labels... 1703 images, 0 backgrounds, 0 corrupt: 100%|██████████| 1703/1703 [00:02<00:00, 756.18it/s] \n","\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/animal-pose-data/valid/images/n02089973_1763.jpg: corrupt JPEG restored and saved\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/animal-pose-data/valid/labels.cache\n","Plotting labels to Animal_Keypoints/yolov8m-pose_100_epochs/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 83 weight(decay=0.0), 93 weight(decay=0.0005), 92 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1mAnimal_Keypoints/yolov8m-pose_100_epochs\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      1/100      6.92G     0.7404      9.976     0.6837     0.7149      1.322          6        640: 100%|██████████| 424/424 [03:53<00:00,  1.82it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:25<00:00,  2.09it/s]\n","                   all       1703       1703      0.925      0.974      0.975       0.83     0.0121     0.0117    0.00268   0.000418\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      2/100      7.03G     0.7318      8.044     0.5963     0.5747        1.3          5        640: 100%|██████████| 424/424 [03:52<00:00,  1.83it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.17it/s]\n","                   all       1703       1703      0.921      0.959      0.971      0.785     0.0771     0.0658     0.0145    0.00206\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      3/100      7.06G     0.8704      7.556     0.5583     0.6886      1.407          6        640: 100%|██████████| 424/424 [03:51<00:00,  1.83it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.21it/s]\n","                   all       1703       1703      0.904        0.9      0.945      0.688      0.133      0.105      0.022    0.00303\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      4/100      7.08G     0.9709      7.355     0.5397     0.7879      1.479         11        640: 100%|██████████| 424/424 [03:51<00:00,  1.83it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.28it/s]\n","                   all       1703       1703       0.89      0.923      0.959      0.702      0.312      0.251      0.138     0.0231\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      5/100      7.06G     0.9476      7.001     0.5274     0.7637      1.455         10        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.26it/s]\n","                   all       1703       1703       0.87      0.858      0.914      0.629      0.222      0.208     0.0844     0.0136\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      6/100      7.05G     0.9081      6.644     0.5158     0.7246      1.415          8        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.30it/s]\n","                   all       1703       1703       0.93      0.945      0.966      0.742      0.431      0.372      0.225     0.0435\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      7/100      7.05G     0.8805      6.395     0.5071     0.6892      1.388          5        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.30it/s]\n","                   all       1703       1703      0.925      0.954      0.978      0.756      0.515      0.431      0.323     0.0644\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      8/100      7.05G     0.8528       6.23      0.502     0.6563      1.368          5        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.31it/s]\n","                   all       1703       1703      0.938      0.955      0.981      0.805      0.545      0.497      0.392     0.0817\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","      9/100      7.07G     0.8008      5.947     0.4954     0.6171       1.32          5        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.27it/s]\n","                   all       1703       1703      0.943      0.954      0.973      0.794      0.593      0.554      0.458      0.106\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     10/100      7.07G     0.8052      5.817     0.4892     0.6159      1.329         12        640: 100%|██████████| 424/424 [03:51<00:00,  1.83it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.20it/s]\n","                   all       1703       1703      0.927      0.943      0.971      0.775      0.625       0.54      0.498      0.129\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     11/100      7.05G     0.7803      5.705     0.4834     0.6012      1.302          6        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.23it/s]\n","                   all       1703       1703      0.935       0.95      0.978      0.796       0.65      0.591      0.505      0.127\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     12/100      7.06G     0.7734      5.612     0.4753     0.5878       1.29         10        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.23it/s]\n","                   all       1703       1703      0.936      0.961      0.971        0.8      0.677      0.656      0.556      0.137\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     13/100      7.05G     0.7504      5.509     0.4711     0.5579       1.27         10        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.26it/s]\n","                   all       1703       1703      0.947      0.959      0.984      0.816      0.665      0.655      0.556      0.149\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     14/100      7.06G     0.7351      5.382     0.4667     0.5396      1.255          6        640: 100%|██████████| 424/424 [03:49<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.29it/s]\n","                   all       1703       1703      0.951      0.951      0.982      0.835      0.704      0.661      0.587       0.16\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     15/100      7.07G     0.7278      5.284       0.46     0.5445      1.253         11        640: 100%|██████████| 424/424 [03:51<00:00,  1.83it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.18it/s]\n","                   all       1703       1703      0.953      0.974      0.983      0.837      0.739      0.713      0.658       0.19\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     16/100      7.06G     0.7107       5.22     0.4539     0.5269      1.235          8        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.20it/s]\n","                   all       1703       1703      0.958      0.963      0.985      0.848      0.751      0.711      0.674      0.198\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     17/100      7.06G     0.6949      5.119     0.4487     0.5166      1.227          9        640: 100%|██████████| 424/424 [03:49<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.21it/s]\n","                   all       1703       1703      0.948      0.967       0.98      0.837      0.764      0.724      0.672        0.2\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     18/100      7.06G     0.6944      5.057     0.4462     0.5192      1.226          9        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.26it/s]\n","                   all       1703       1703      0.956      0.962      0.986      0.847      0.768      0.752      0.704      0.213\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     19/100      7.06G      0.676      4.939       0.44     0.4961      1.212          9        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.23it/s]\n","                   all       1703       1703       0.95      0.975      0.984      0.847      0.762      0.736      0.687      0.204\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     20/100      7.06G     0.6669      4.954     0.4379     0.5019      1.202          9        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.21it/s]\n","                   all       1703       1703       0.95      0.971      0.985      0.847      0.779      0.774      0.724      0.225\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     21/100      7.05G     0.6592      4.797     0.4368     0.4864      1.188          8        640: 100%|██████████| 424/424 [03:49<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.20it/s]\n","                   all       1703       1703      0.965      0.961      0.986      0.857      0.802      0.782      0.748      0.247\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     22/100      7.06G     0.6607      4.742     0.4305     0.4857      1.196          5        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.30it/s]\n","                   all       1703       1703      0.952      0.977      0.986      0.857      0.789      0.764       0.73       0.23\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     23/100      7.06G      0.645      4.726     0.4254     0.4806      1.184          9        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.28it/s]\n","                   all       1703       1703      0.958      0.969      0.985      0.867       0.81      0.786      0.747      0.252\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     24/100      7.07G     0.6467      4.617      0.426     0.4762      1.183          5        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.32it/s]\n","                   all       1703       1703      0.953      0.972      0.986      0.859      0.808      0.796       0.76       0.26\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     25/100      7.06G     0.6209      4.575     0.4228     0.4586      1.162          8        640: 100%|██████████| 424/424 [03:49<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.28it/s]\n","                   all       1703       1703      0.955      0.971      0.987      0.862      0.823      0.797      0.765      0.257\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     26/100      7.06G     0.6241      4.556     0.4214     0.4465      1.171         13        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.19it/s]\n","                   all       1703       1703      0.963      0.975      0.989      0.872      0.824      0.804      0.771      0.264\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     27/100      7.07G     0.6112      4.512     0.4189     0.4453      1.154         11        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.20it/s]\n","                   all       1703       1703      0.957      0.979      0.986      0.867      0.829      0.812      0.793      0.288\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     28/100      7.05G     0.6194      4.455     0.4162     0.4542      1.163          9        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.21it/s]\n","                   all       1703       1703       0.96      0.976      0.988      0.877      0.824      0.816      0.777      0.277\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     29/100      7.05G     0.6054      4.402     0.4146     0.4396      1.155          5        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.22it/s]\n","                   all       1703       1703      0.946      0.986      0.989      0.885      0.839      0.834      0.828      0.319\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     30/100      7.07G     0.6067      4.406     0.4141     0.4437      1.152          8        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.21it/s]\n","                   all       1703       1703      0.962      0.979      0.988      0.883      0.858      0.832      0.826      0.317\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     31/100      7.07G     0.5939      4.333     0.4153      0.436      1.143          7        640: 100%|██████████| 424/424 [03:49<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.21it/s]\n","                   all       1703       1703      0.955      0.979      0.986      0.883      0.842      0.828      0.806      0.297\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     32/100      7.07G      0.591      4.268     0.4113       0.43      1.142         12        640: 100%|██████████| 424/424 [03:49<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.24it/s]\n","                   all       1703       1703      0.961      0.975      0.986      0.885      0.836      0.835      0.805      0.307\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     33/100      7.07G     0.5822      4.224     0.4078      0.419      1.133         10        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.21it/s]\n","                   all       1703       1703      0.963      0.975      0.988      0.887      0.859      0.843      0.837      0.328\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     34/100      7.05G     0.5791      4.216     0.4066      0.421      1.129         11        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.25it/s]\n","                   all       1703       1703      0.961      0.978      0.989      0.888      0.869      0.858       0.85      0.337\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     35/100      7.06G     0.5789      4.198     0.4059     0.4196      1.128         11        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.26it/s]\n","                   all       1703       1703      0.958      0.972      0.989      0.888      0.862      0.842      0.829      0.332\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     36/100      7.07G     0.5692      4.126     0.4043     0.4112       1.12         10        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.25it/s]\n","                   all       1703       1703      0.961      0.968      0.988      0.893      0.868      0.849      0.842      0.344\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     37/100      7.05G     0.5697      4.139     0.4017     0.4162      1.121          6        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.22it/s]\n","                   all       1703       1703      0.964       0.97       0.99       0.89      0.878      0.857      0.847      0.344\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     38/100      7.05G      0.562      4.073     0.4023      0.408      1.118          9        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.30it/s]\n","                   all       1703       1703       0.96       0.98       0.99      0.888      0.877      0.853      0.852      0.349\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     39/100      7.05G     0.5629      4.043     0.4023     0.4002      1.115          6        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.18it/s]\n","                   all       1703       1703      0.968      0.971       0.99      0.888      0.871      0.858      0.842      0.342\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     40/100      7.08G     0.5609      3.987     0.3964     0.4005      1.116          5        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:25<00:00,  2.15it/s]\n","                   all       1703       1703      0.967      0.973       0.99        0.9      0.879      0.861      0.854      0.351\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     41/100      7.05G      0.546      3.964     0.3952     0.3965      1.104         11        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.16it/s]\n","                   all       1703       1703      0.962      0.975       0.99      0.891      0.874      0.861      0.843      0.347\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     42/100      7.06G     0.5497      3.912      0.397     0.3955      1.109         10        640: 100%|██████████| 424/424 [03:51<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.24it/s]\n","                   all       1703       1703      0.959      0.976       0.99      0.898      0.884      0.876       0.87      0.369\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     43/100      7.07G     0.5465      3.941     0.3961     0.3975      1.101         11        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.18it/s]\n","                   all       1703       1703      0.961      0.978      0.991      0.897      0.884      0.881      0.877      0.374\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     44/100      7.05G     0.5378      3.933     0.3914     0.3872      1.092          9        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:25<00:00,  2.16it/s]\n","                   all       1703       1703      0.972      0.971       0.99      0.898      0.883       0.88      0.869      0.371\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     45/100      7.06G     0.5301      3.836     0.3906     0.3804      1.094         11        640: 100%|██████████| 424/424 [03:49<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.19it/s]\n","                   all       1703       1703      0.969      0.973       0.99      0.896      0.885       0.88      0.873      0.378\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     46/100      7.06G     0.5267      3.822      0.391     0.3799      1.087         11        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:23<00:00,  2.31it/s]\n","                   all       1703       1703      0.968      0.975      0.991      0.903      0.891      0.874      0.882      0.388\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     47/100      7.07G     0.5259      3.736     0.3893     0.3715      1.088         14        640: 100%|██████████| 424/424 [03:49<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.19it/s]\n","                   all       1703       1703      0.963       0.98      0.991      0.903      0.901      0.877      0.886      0.388\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     48/100      7.08G     0.5204      3.762     0.3904     0.3674      1.086          7        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.19it/s]\n","                   all       1703       1703      0.973      0.975      0.991      0.905      0.894      0.883      0.881      0.392\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     49/100      7.06G     0.5269      3.769     0.3889     0.3713      1.082          8        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.20it/s]\n","                   all       1703       1703      0.965      0.979      0.991      0.906      0.905      0.884      0.889      0.399\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     50/100      7.07G     0.5288      3.702      0.386      0.374      1.088          7        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.20it/s]\n","                   all       1703       1703       0.96      0.981       0.99      0.904      0.896      0.883       0.89      0.399\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     51/100      7.06G     0.5127      3.698     0.3869     0.3651      1.077          6        640: 100%|██████████| 424/424 [03:49<00:00,  1.85it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.21it/s]\n","                   all       1703       1703      0.972      0.966      0.991      0.906      0.895      0.889      0.889      0.401\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     52/100      7.07G     0.5207      3.686     0.3845     0.3754      1.086          8        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.22it/s]\n","                   all       1703       1703      0.971      0.972      0.991      0.913      0.908      0.895      0.901      0.415\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     53/100      7.07G     0.5137      3.624     0.3835     0.3612      1.078          7        640: 100%|██████████| 424/424 [03:50<00:00,  1.84it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|██████████| 54/54 [00:24<00:00,  2.19it/s]\n","                   all       1703       1703      0.972      0.974      0.991      0.907      0.904      0.894      0.896      0.419\n","\n","      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n","     54/100      7.07G     0.4972      3.684     0.3854      0.356       1.06         26        640:  51%|█████▏    | 218/424 [01:59<01:48,  1.91it/s]"]}],"source":["pose_model = model = YOLO(train_config.MODEL)\n","\n","pose_model.train(data    = train_config.DATASET_YAML,\n","            epochs       = train_config.EPOCHS,\n","            imgsz        = data_config.IMAGE_SIZE,\n","            batch        = data_config.BATCH_SIZE,\n","            project      = train_config.PROJECT,\n","            name         = train_config.NAME,\n","            close_mosaic = data_config.CLOSE_MOSAIC,\n","            mosaic       = data_config.MOSAIC,\n","            fliplr       = data_config.FLIP_LR\n","           )"]},{"cell_type":"markdown","id":"6d9e1adb-eaae-44f1-880a-e7544e7b6f6a","metadata":{"id":"6d9e1adb-eaae-44f1-880a-e7544e7b6f6a"},"source":["## 7 Evaluation"]},{"cell_type":"code","execution_count":null,"id":"506e12a5-c16d-401a-98e8-f1cd45ac05f1","metadata":{"id":"506e12a5-c16d-401a-98e8-f1cd45ac05f1"},"outputs":[],"source":["ckpt_path  = os.path.join(train_config.PROJECT, train_config.NAME, \"weights\", \"best.pt\")\n","model_pose = YOLO(ckpt_path)\n","\n","metrics = model_pose.val()"]},{"cell_type":"markdown","id":"b8f18c87-bb0c-45f6-a499-0a2086073fe6","metadata":{"id":"b8f18c87-bb0c-45f6-a499-0a2086073fe6"},"source":["## 8 Predictions\n","\n","The `prepare_predictions` function obtains the predicted boxes, confidence scores, and keypoints for the corresponding image."]},{"cell_type":"code","execution_count":null,"id":"802f8f3b-263e-443b-944a-057b935c188a","metadata":{"id":"802f8f3b-263e-443b-944a-057b935c188a"},"outputs":[],"source":["def prepare_predictions(\n","    image_dir_path,\n","    image_filename,\n","    model,\n","    BOX_IOU_THRESH = 0.55,\n","    BOX_CONF_THRESH=0.30,\n","    KPT_CONF_THRESH=0.68):\n","\n","    image_path = os.path.join(image_dir_path, image_filename)\n","    image = cv2.imread(image_path).copy()\n","\n","    results = model.predict(image_path, conf=BOX_CONF_THRESH, iou=BOX_IOU_THRESH)[0].cpu()\n","\n","    if not len(results.boxes.xyxy):\n","        return image\n","\n","    # Get the predicted boxes, conf scores and keypoints.\n","    pred_boxes = results.boxes.xyxy.numpy()\n","    pred_box_conf = results.boxes.conf.numpy()\n","    pred_kpts_xy = results.keypoints.xy.numpy()\n","    pred_kpts_conf = results.keypoints.conf.numpy()\n","\n","    # Draw predicted bounding boxes, conf scores and keypoints on image.\n","    for boxes, score, kpts, confs in zip(pred_boxes, pred_box_conf, pred_kpts_xy, pred_kpts_conf):\n","        kpts_ids = np.where(confs > KPT_CONF_THRESH)[0]\n","        filter_kpts = kpts[kpts_ids]\n","        filter_kpts = np.concatenate([filter_kpts, np.expand_dims(kpts_ids, axis=-1)], axis=-1)\n","        image = draw_boxes(image, boxes, score=score)\n","        image = draw_landmarks(image, filter_kpts)\n","\n","    return image"]},{"cell_type":"code","execution_count":null,"id":"97c7de57-709d-4ae3-8345-f6673b4d34d0","metadata":{"id":"97c7de57-709d-4ae3-8345-f6673b4d34d0"},"outputs":[],"source":["VAL_IMAGE_FILES = os.listdir(VALID_IMG_PATH)\n","\n","num_samples = 9\n","num_rows = 3\n","num_cols = num_samples//num_rows\n","\n","fig, ax = plt.subplots(\n","        nrows=num_rows,\n","        ncols=num_cols,\n","        figsize=(25, 15),\n","    )\n","\n","random.seed(90)\n","random.shuffle(VAL_IMAGE_FILES)\n","\n","for idx, (file, axis) in enumerate(zip(VAL_IMAGE_FILES[:num_samples], ax.flat)):\n","\n","    image_pred = prepare_predictions(VALID_IMG_PATH, file, model_pose)\n","    axis.imshow(image_pred[...,::-1])\n","    axis.axis(\"off\")\n","\n","plt.tight_layout(h_pad=4., w_pad=4.)\n","plt.show();"]},{"cell_type":"code","execution_count":null,"id":"38b2eeae","metadata":{"id":"38b2eeae"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"ultralytics","language":"python","name":"ultralytics"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}